{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3d3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e087bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbp = pd.read_csv('../../Data/play-by-play/pbp_exp12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns\n",
    "categorical_cols = [\n",
    "    'OffenseTeam', 'DefenseTeam', 'play_type', 'side_of_field', 'stadium', \n",
    "    'play_type_nfl', 'roof', 'surface_type', 'home_team', 'away_team', \n",
    "    'season_type', 'offense_formation', 'temperature_grade', 'temperature_bucket'\n",
    "]\n",
    "\n",
    "# Dictionary to hold LabelEncoders for each column\n",
    "label_encoders = {}\n",
    "\n",
    "# Apply LabelEncoder to each categorical column and store encoders\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    pbp[col] = le.fit_transform(pbp[col].astype(str))  # Ensure the column is treated as a string\n",
    "    label_encoders[col] = le "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7796b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pbp[['week', 'OffenseTeam', 'DefenseTeam', 'Down', 'YardsToGo', 'yardline_100', 'play_type',\n",
    "               'Quarter', 'game_seconds_remaining', 'game_half', 'drive', 'series', 'score_differential',\n",
    "               'stadium', 'roof', 'surface_type', 'HOME_day_since_last_game', 'AWAY_day_since_last_game',\n",
    "               'total_play_count', 'home_team_on_offense', 'home_team_on_defense', 'season_progression',\n",
    "               'defenders_in_box', 'offense_formation', 'Temperature', 'Precipitation', 'goal_line_situation',\n",
    "               'pass_count', 'run_count', 'current_defence_rank','temperature_grade', 'epa', \n",
    "               'late_season','is_close_game', 'poor_field_condition', 'rest_differential', 'blitz_situation',\n",
    "               'short_rest','pass_run_ratio', 'offensive_predictability', 'fourth_down_attempt', 'third_and_long']]\n",
    "\n",
    "\n",
    "y = pbp['Player-Injured-On-Play']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 53  \n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists for storing metrics and models\n",
    "accuracy_scores, precision_scores, recall_scores = [], [], []\n",
    "false_positive_cases, false_negative_cases = [], []\n",
    "fold_models = {}\n",
    "\n",
    "\n",
    "# --- Cross-Validation Loop ---\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Separate injured and non-injured plays\n",
    "    X_train_pos = X_train[y_train == 1]\n",
    "    X_train_neg = X_train[y_train == 0]\n",
    "    y_train_pos = y_train[y_train == 1]\n",
    "    y_train_neg = y_train[y_train == 0]\n",
    "\n",
    "    X_test_pos = X_test[y_test == 1]\n",
    "    X_test_neg = X_test[y_test == 0]\n",
    "    y_test_pos = y_test[y_test == 1]\n",
    "    y_test_neg = y_test[y_test == 0]\n",
    "\n",
    "    # Resample to ensure balance\n",
    "    X_train_pos_resampled, y_train_pos_resampled = resample(X_train_pos, y_train_pos, replace=True, n_samples=1619, random_state=42)\n",
    "    X_train_neg_resampled, y_train_neg_resampled = resample(X_train_neg, y_train_neg, replace=False, n_samples=1619, random_state=42)\n",
    "\n",
    "    X_test_pos_resampled, y_test_pos_resampled = resample(X_test_pos, y_test_pos, replace=True, n_samples=1619, random_state=42)\n",
    "    X_test_neg_resampled, y_test_neg_resampled = resample(X_test_neg, y_test_neg, replace=False, n_samples=1619, random_state=42)\n",
    "\n",
    "    # Combine resampled data\n",
    "    X_train_resampled = pd.concat([X_train_pos_resampled, X_train_neg_resampled])\n",
    "    y_train_resampled = pd.concat([y_train_pos_resampled, y_train_neg_resampled])\n",
    "\n",
    "    X_test_resampled = pd.concat([X_test_pos_resampled, X_test_neg_resampled])\n",
    "    y_test_resampled = pd.concat([y_test_pos_resampled, y_test_neg_resampled])\n",
    "\n",
    "    weights = pd.Series(1, index=X_train_resampled.index)\n",
    "\n",
    "    # Weighting conditions\n",
    "    weights[X_train_resampled['defenders_in_box'] > 2.5] += 1\n",
    "    weights[X_train_resampled['epa'] <= 1] += 1\n",
    "    weights[X_train_resampled['play_type'] > 7.5] += 1\n",
    "    weights[X_train_resampled['game_seconds_remaining'] > 2300 ] -= 1\n",
    "\n",
    "    # Train Decision Tree\n",
    "    dt_model = DecisionTreeClassifier(random_state=42, max_depth=6)\n",
    "    dt_model.fit(X_train_resampled, y_train_resampled, sample_weight=weights)\n",
    "\n",
    "    # Predict & Evaluate\n",
    "    y_pred = dt_model.predict(X_test_resampled)\n",
    "    accuracy_scores.append(accuracy_score(y_test_resampled, y_pred))\n",
    "    precision_scores.append(precision_score(y_test_resampled, y_pred))\n",
    "    recall_scores.append(recall_score(y_test_resampled, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eec3ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.5807\n",
      "Min Accuracy: 0.4833\n",
      "Max Accuracy: 0.6640\n",
      "Mean Precision: 0.5628\n",
      "Min Precision: 0.4844\n",
      "Max Precision: 0.6374\n",
      "Mean Recall: 0.7110\n",
      "Min Recall: 0.4466\n",
      "Max Recall: 0.8814\n"
     ]
    }
   ],
   "source": [
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"Min Accuracy: {min(accuracy_scores):.4f}\")\n",
    "print(f\"Max Accuracy: {max(accuracy_scores):.4f}\")\n",
    "\n",
    "mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "print(f\"Min Precision: {min(precision_scores):.4f}\")\n",
    "print(f\"Max Precision: {max(precision_scores):.4f}\")\n",
    "\n",
    "mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "print(f\"Min Recall: {min(recall_scores):.4f}\")\n",
    "print(f\"Max Recall: {max(recall_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3c291",
   "metadata": {},
   "source": [
    "**Mean Accuracy:** 0.5696  \n",
    "**Mean Precision:** 0.5543  \n",
    "**Mean Recall:** 0.6853"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
